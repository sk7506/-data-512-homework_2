{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Article Page Info MediaWiki API Example\n",
    "This example illustrates how to access page info data using the [MediaWiki REST API for the EN Wikipedia](https://www.mediawiki.org/wiki/API:Main_page). This example shows how to request summary 'page info' for a single article page. The API documentation, [API:Info](https://www.mediawiki.org/wiki/API:Info), covers additional details that may be helpful when trying to use or understand this example.\n",
    "\n",
    "## License\n",
    "This code example was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.2 - September 16, 2024\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Libraries\n",
    "\n",
    "The following code block references Dr. David W. Mcdonald's example code, with new libraries added to the originals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "# These are standard python modules\n",
    "import json, time, urllib.parse\n",
    "#\n",
    "# The 'requests' module is not a standard Python module. You will need to install this with pip/pip3 if you do not already have it\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "\n",
    "Imports two csv files containing (1) information about politicians with Wikipedia article titles (and their respective countries) and (2) populations in the millions and corresponding regions of countries.\n",
    "\n",
    "These article titles are titles of Wikipedia pages about a politician, at varying degrees of completeness.\n",
    "\n",
    "The following code block references Dr. David W. Mcdonald's example code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Majah Ha Adrif', 'Haroon al-Afghani', 'Tayyab Agha', 'Khadija Zahra Ahmadi', 'Aziza Ahmadyar', 'Muqadasa Ahmadzai', 'Mohammad Sarwar Ahmedzai', 'Amir Muhammad Akhundzada', 'Nasrullah Baryalai Arsalai', 'Abdul Rahim Ayoubi']\n"
     ]
    }
   ],
   "source": [
    "# Code to load csv file containing article names into a dataframe and iterate into the ARTICLE_TITLES list\n",
    "\n",
    "politicians_by_country = pd.read_csv('politicians_by_country_AUG.2024.csv')\n",
    "pop_by_country = pd.read_csv('population_by_country_AUG.2024.csv')\n",
    "\n",
    "\n",
    "ARTICLE_TITLES = politicians_by_country['name'][0:].tolist()\n",
    "\n",
    "# Display the resulting list\n",
    "print(ARTICLE_TITLES[0:10])\n",
    "\n",
    "# first, input the article titles of politician names into page_info_api\n",
    "# from the api result, take the revision ID and input revision ID into ORES Api\n",
    "# then, get output of article quality score from ORES and merge the score with the population dataset\n",
    "# then combine into a csv file with specific column names\n",
    "# then, analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Constants\n",
    "\n",
    "Establishes the API request endpoint and parameters in order to extract the Wikipedia page revision IDs. \n",
    "\n",
    "The following code block references Dr. David W. Mcdonald's example code.\n",
    "\n",
    "##### USER NOTE! \n",
    "- Toggle the email to the corresponding student's email\n",
    "- specify the article titles source, or use the example commented out below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "API_HEADER_AGENT = 'User-Agent'\n",
    "\n",
    "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<kilpas@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2024'\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests\n",
    "# ARTICLE_TITLES = [ 'Bison', 'Northern flicker', 'Red squirrel', 'Chinook salmon', 'Horseshoe bat' ]\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling the API\n",
    "\n",
    "The API request will be made using one procedure. The idea is to make this reusable. The procedure is parameterized, but relies on the constants above for the important parameters. The underlying assumption is that this will be used to request data for a set of article pages. Therefore the parameter most likely to change is the article_title.\n",
    "\n",
    "The following code block references Dr. David W. Mcdonald's example code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_pageinfo_per_articles(article_titles, \n",
    "                                  endpoint_url=API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                  request_template=PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                  headers=REQUEST_HEADERS):\n",
    "    \n",
    "    # Make sure headers contain the correct User-Agent information\n",
    "    if 'kilpas@uw' in headers[API_HEADER_AGENT]:\n",
    "        raise Exception(f\"Use your UW email address in the '{API_HEADER_AGENT}' field.\")\n",
    "    \n",
    "    # Break the article titles list into chunks of 50 (API limit)\n",
    "    article_batches = [article_titles[i:i + MAX_TITLES_PER_REQUEST] for i in range(0, len(article_titles), MAX_TITLES_PER_REQUEST)]\n",
    "    \n",
    "    all_pages_info = {}\n",
    "    \n",
    "    for batch in article_batches:\n",
    "        page_titles = \"|\".join(batch)\n",
    "        \n",
    "        # Update the template with the current batch of titles\n",
    "        request_template['titles'] = page_titles\n",
    "        \n",
    "        # Make the request\n",
    "        try:\n",
    "            # Throttle the requests to avoid overloading the API\n",
    "            if API_THROTTLE_WAIT > 0.0:\n",
    "                time.sleep(API_THROTTLE_WAIT)\n",
    "                \n",
    "            response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "            json_response = response.json()\n",
    "            \n",
    "            if json_response and 'query' in json_response:\n",
    "                pages = json_response['query']['pages']\n",
    "                all_pages_info.update(pages)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while requesting data for titles: {page_titles}\\nError: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return all_pages_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a way to get the information for multiple pages at the same time, by separating the page titles with the vertical bar \"|\" character. However, this approach has limits. You should probably check the API documentation if you want to do multiple pages in a single request - and limit the number of pages in one request reasonably.\n",
    "\n",
    "This example also illustrates creating a copy of the template, setting values in the template, and then calling the function using the template to supply the parameters for the API request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check - Article Titles Extraction\n",
    "\n",
    "This block of code ensures the article titles are being correctly extracted before finding the Revision IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Majah Ha Adrif', 'Haroon al-Afghani', 'Tayyab Agha', 'Khadija Zahra Ahmadi', 'Aziza Ahmadyar', 'Muqadasa Ahmadzai', 'Mohammad Sarwar Ahmedzai', 'Amir Muhammad Akhundzada', 'Nasrullah Baryalai Arsalai', 'Abdul Rahim Ayoubi']\n"
     ]
    }
   ],
   "source": [
    "# Load data from the CSV files\n",
    "politicians_by_country = pd.read_csv('politicians_by_country_AUG.2024.csv')\n",
    "pop_by_country = pd.read_csv('population_by_country_AUG.2024.csv')\n",
    "\n",
    "# Remove duplicates in the 'name' column (which is the first column)\n",
    "politicians_by_country_cleaned = politicians_by_country.iloc[:, 0].drop_duplicates()\n",
    "\n",
    "# Extract the article titles from the cleaned data (first column)\n",
    "ARTICLE_TITLES = politicians_by_country_cleaned.tolist()\n",
    "\n",
    "# Display the first 10 titles (just to verify the extraction)\n",
    "print(ARTICLE_TITLES[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying Constants for the Student Example\n",
    "\n",
    "This block of code mirrors the professor's example above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "ORES_API_ENDPOINT = \"https://ores.wikimedia.org/v3/scores/{}/?models=draftquality&revids={}\"\n",
    "\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<kilpas@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2024'\n",
    "}\n",
    "\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers|lastrevid\"\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n",
    "\n",
    "MAX_TITLES_PER_REQUEST = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Function to Request API Page Info\n",
    "\n",
    "Edits the function above to call the API in a more efficient manner, using the \"|\" character and outputting a json file with article titles and corresponding revision IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politicians and their revision IDs have been saved to 'politicians-and-revision-ids.json'.\n"
     ]
    }
   ],
   "source": [
    "def request_pageinfo_per_articles(article_titles, \n",
    "                                   endpoint_url=API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                   request_template=PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                   headers=REQUEST_HEADERS):\n",
    "    \"\"\"\n",
    "    Request page info for a list of article titles in batches of 50.\n",
    "    Returns a dictionary with politician names as keys and their last revision IDs as values.\n",
    "    \"\"\"\n",
    "    if 'uwnetid@uw.edu' in headers['User-Agent']:\n",
    "        raise Exception(\"Please replace 'uwnetid@uw.edu' with your actual UW email address in the headers.\")\n",
    "    \n",
    "    article_batches = [article_titles[i:i + MAX_TITLES_PER_REQUEST] for i in range(0, len(article_titles), MAX_TITLES_PER_REQUEST)]\n",
    "    politicians_revisions = {}  # Dictionary to hold politician names and their last revision IDs\n",
    "    \n",
    "    for batch in article_batches:\n",
    "        page_titles = \"|\".join(batch)\n",
    "        request_template['titles'] = page_titles\n",
    "        \n",
    "        try:\n",
    "            time.sleep(0.1)  # Throttle requests\n",
    "            response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "            json_response = response.json()\n",
    "            \n",
    "            if json_response and 'query' in json_response:\n",
    "                pages = json_response['query']['pages']\n",
    "                for page_id, page_data in pages.items():\n",
    "                    politician_name = page_data.get('title')\n",
    "                    last_revision_id = page_data.get('lastrevid')\n",
    "                    if politician_name and last_revision_id:\n",
    "                        # Store only title and lastrevid in the output dictionary\n",
    "                        politicians_revisions[politician_name] = {\n",
    "                            'title': politician_name,\n",
    "                            'lastrevid': last_revision_id\n",
    "                        }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching page info for titles: {page_titles}\\nError: {e}\")\n",
    "    \n",
    "    return politicians_revisions\n",
    "\n",
    "# Step 1: Get page info for each politician (including revision ID)\n",
    "politicians_revisions = request_pageinfo_per_articles(ARTICLE_TITLES)\n",
    "\n",
    "# Step 2: Write the data to a JSON file\n",
    "with open('politicians-and-revision-ids.json', 'w') as json_file:\n",
    "    json.dump(politicians_revisions, json_file, indent=4)\n",
    "\n",
    "print(\"Politicians and their revision IDs have been saved to 'politicians-and-revision-ids.json'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
