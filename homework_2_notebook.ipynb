{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Article Page Info MediaWiki API Example\n",
    "This example illustrates how to access page info data using the [MediaWiki REST API for the EN Wikipedia](https://www.mediawiki.org/wiki/API:Main_page). This example shows how to request summary 'page info' for a single article page. The API documentation, [API:Info](https://www.mediawiki.org/wiki/API:Info), covers additional details that may be helpful when trying to use or understand this example.\n",
    "\n",
    "## License\n",
    "This code example was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.2 - September 16, 2024\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "# These are standard python modules\n",
    "import json, time, urllib.parse\n",
    "#\n",
    "# The 'requests' module is not a standard Python module. You will need to install this with pip/pip3 if you do not already have it\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Majah Ha Adrif', 'Haroon al-Afghani', 'Tayyab Agha', 'Khadija Zahra Ahmadi', 'Aziza Ahmadyar', 'Muqadasa Ahmadzai', 'Mohammad Sarwar Ahmedzai', 'Amir Muhammad Akhundzada', 'Nasrullah Baryalai Arsalai', 'Abdul Rahim Ayoubi']\n"
     ]
    }
   ],
   "source": [
    "# Code to load csv file containing article names into a dataframe and iterate into the ARTICLE_TITLES list\n",
    "\n",
    "politicians_by_country = pd.read_csv('politicians_by_country_AUG.2024.csv')\n",
    "pop_by_country = pd.read_csv('population_by_country_AUG.2024.csv')\n",
    "\n",
    "\n",
    "ARTICLE_TITLES = politicians_by_country['name'][0:].tolist()\n",
    "\n",
    "# Display the resulting list\n",
    "print(ARTICLE_TITLES[0:10])\n",
    "\n",
    "# first, input the article titles of politician names into page_info_api\n",
    "# from the api result, take the revision ID and input revision ID into ORES Api\n",
    "# then, get output of article quality score from ORES and merge the score with the population dataset\n",
    "# then combine into a csv file with specific column names\n",
    "# then, analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example relies on some constants that help make the code a bit more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "API_HEADER_AGENT = 'User-Agent'\n",
    "\n",
    "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<kilpas@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2024'\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests\n",
    "ARTICLE_TITLES = [ 'Bison', 'Northern flicker', 'Red squirrel', 'Chinook salmon', 'Horseshoe bat' ]\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API request will be made using one procedure. The idea is to make this reusable. The procedure is parameterized, but relies on the constants above for the important parameters. The underlying assumption is that this will be used to request data for a set of article pages. Therefore the parameter most likely to change is the article_title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_pageinfo_per_articles(article_titles, \n",
    "                                  endpoint_url=API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                  request_template=PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                  headers=REQUEST_HEADERS):\n",
    "    \n",
    "    # Make sure headers contain the correct User-Agent information\n",
    "    if 'kilpas@uw' in headers[API_HEADER_AGENT]:\n",
    "        raise Exception(f\"Use your UW email address in the '{API_HEADER_AGENT}' field.\")\n",
    "    \n",
    "    # Break the article titles list into chunks of 50 (API limit)\n",
    "    article_batches = [article_titles[i:i + MAX_TITLES_PER_REQUEST] for i in range(0, len(article_titles), MAX_TITLES_PER_REQUEST)]\n",
    "    \n",
    "    all_pages_info = {}\n",
    "    \n",
    "    for batch in article_batches:\n",
    "        page_titles = \"|\".join(batch)\n",
    "        \n",
    "        # Update the template with the current batch of titles\n",
    "        request_template['titles'] = page_titles\n",
    "        \n",
    "        # Make the request\n",
    "        try:\n",
    "            # Throttle the requests to avoid overloading the API\n",
    "            if API_THROTTLE_WAIT > 0.0:\n",
    "                time.sleep(API_THROTTLE_WAIT)\n",
    "                \n",
    "            response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "            json_response = response.json()\n",
    "            \n",
    "            if json_response and 'query' in json_response:\n",
    "                pages = json_response['query']['pages']\n",
    "                all_pages_info.update(pages)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while requesting data for titles: {page_titles}\\nError: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return all_pages_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page info data for: Chinook salmon\n",
      "{\n",
      "    \"batchcomplete\": \"\",\n",
      "    \"query\": {\n",
      "        \"pages\": {\n",
      "            \"1212891\": {\n",
      "                \"pageid\": 1212891,\n",
      "                \"ns\": 0,\n",
      "                \"title\": \"Chinook salmon\",\n",
      "                \"contentmodel\": \"wikitext\",\n",
      "                \"pagelanguage\": \"en\",\n",
      "                \"pagelanguagehtmlcode\": \"en\",\n",
      "                \"pagelanguagedir\": \"ltr\",\n",
      "                \"touched\": \"2024-08-16T10:34:52Z\",\n",
      "                \"lastrevid\": 1234351318,\n",
      "                \"length\": 53787,\n",
      "                \"watchers\": 108,\n",
      "                \"talkid\": 3909817,\n",
      "                \"fullurl\": \"https://en.wikipedia.org/wiki/Chinook_salmon\",\n",
      "                \"editurl\": \"https://en.wikipedia.org/w/index.php?title=Chinook_salmon&action=edit\",\n",
      "                \"canonicalurl\": \"https://en.wikipedia.org/wiki/Chinook_salmon\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Getting page info data for: {ARTICLE_TITLES[3]}\")\n",
    "info = request_pageinfo_per_article(ARTICLE_TITLES[3])\n",
    "print(json.dumps(info,indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page info data for: Northern flicker\n",
      "{\n",
      "    \"351590\": {\n",
      "        \"pageid\": 351590,\n",
      "        \"ns\": 0,\n",
      "        \"title\": \"Northern flicker\",\n",
      "        \"contentmodel\": \"wikitext\",\n",
      "        \"pagelanguage\": \"en\",\n",
      "        \"pagelanguagehtmlcode\": \"en\",\n",
      "        \"pagelanguagedir\": \"ltr\",\n",
      "        \"touched\": \"2024-08-16T10:34:47Z\",\n",
      "        \"lastrevid\": 1237967172,\n",
      "        \"length\": 32225,\n",
      "        \"watchers\": 113,\n",
      "        \"talkid\": 8324488,\n",
      "        \"fullurl\": \"https://en.wikipedia.org/wiki/Northern_flicker\",\n",
      "        \"editurl\": \"https://en.wikipedia.org/w/index.php?title=Northern_flicker&action=edit\",\n",
      "        \"canonicalurl\": \"https://en.wikipedia.org/wiki/Northern_flicker\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Getting page info data for: {ARTICLE_TITLES[1]}\")\n",
    "info = request_pageinfo_per_article(ARTICLE_TITLES[1])\n",
    "print(json.dumps(info['query']['pages'],indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a way to get the information for multiple pages at the same time, by separating the page titles with the vertical bar \"|\" character. However, this approach has limits. You should probably check the API documentation if you want to do multiple pages in a single request - and limit the number of pages in one request reasonably.\n",
    "\n",
    "This example also illustrates creating a copy of the template, setting values in the template, and then calling the function using the template to supply the parameters for the API request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page info data for: Bison|Red squirrel|Horseshoe bat\n",
      "{\n",
      "    \"4583\": {\n",
      "        \"pageid\": 4583,\n",
      "        \"ns\": 0,\n",
      "        \"title\": \"Bison\",\n",
      "        \"contentmodel\": \"wikitext\",\n",
      "        \"pagelanguage\": \"en\",\n",
      "        \"pagelanguagehtmlcode\": \"en\",\n",
      "        \"pagelanguagedir\": \"ltr\",\n",
      "        \"touched\": \"2024-08-16T10:34:45Z\",\n",
      "        \"lastrevid\": 1238698333,\n",
      "        \"length\": 60744,\n",
      "        \"watchers\": 259,\n",
      "        \"talkid\": 75239,\n",
      "        \"fullurl\": \"https://en.wikipedia.org/wiki/Bison\",\n",
      "        \"editurl\": \"https://en.wikipedia.org/w/index.php?title=Bison&action=edit\",\n",
      "        \"canonicalurl\": \"https://en.wikipedia.org/wiki/Bison\"\n",
      "    },\n",
      "    \"531505\": {\n",
      "        \"pageid\": 531505,\n",
      "        \"ns\": 0,\n",
      "        \"title\": \"Horseshoe bat\",\n",
      "        \"contentmodel\": \"wikitext\",\n",
      "        \"pagelanguage\": \"en\",\n",
      "        \"pagelanguagehtmlcode\": \"en\",\n",
      "        \"pagelanguagedir\": \"ltr\",\n",
      "        \"touched\": \"2024-08-16T10:34:48Z\",\n",
      "        \"lastrevid\": 1228572241,\n",
      "        \"length\": 57108,\n",
      "        \"watchers\": 64,\n",
      "        \"talkid\": 11206664,\n",
      "        \"fullurl\": \"https://en.wikipedia.org/wiki/Horseshoe_bat\",\n",
      "        \"editurl\": \"https://en.wikipedia.org/w/index.php?title=Horseshoe_bat&action=edit\",\n",
      "        \"canonicalurl\": \"https://en.wikipedia.org/wiki/Horseshoe_bat\"\n",
      "    },\n",
      "    \"638291\": {\n",
      "        \"pageid\": 638291,\n",
      "        \"ns\": 0,\n",
      "        \"title\": \"Red squirrel\",\n",
      "        \"contentmodel\": \"wikitext\",\n",
      "        \"pagelanguage\": \"en\",\n",
      "        \"pagelanguagehtmlcode\": \"en\",\n",
      "        \"pagelanguagedir\": \"ltr\",\n",
      "        \"touched\": \"2024-08-16T10:34:49Z\",\n",
      "        \"lastrevid\": 1239723458,\n",
      "        \"length\": 35652,\n",
      "        \"watchers\": 142,\n",
      "        \"talkid\": 2307431,\n",
      "        \"fullurl\": \"https://en.wikipedia.org/wiki/Red_squirrel\",\n",
      "        \"editurl\": \"https://en.wikipedia.org/w/index.php?title=Red_squirrel&action=edit\",\n",
      "        \"canonicalurl\": \"https://en.wikipedia.org/wiki/Red_squirrel\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "page_titles = f\"{ARTICLE_TITLES[0]}|{ARTICLE_TITLES[2]}|{ARTICLE_TITLES[4]}\"\n",
    "print(f\"Getting page info data for: {page_titles}\")\n",
    "request_info = PAGEINFO_PARAMS_TEMPLATE.copy()\n",
    "request_info['titles'] = page_titles\n",
    "info = request_pageinfo_per_article(request_template=request_info)\n",
    "print(json.dumps(info['query']['pages'],indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Majah Ha Adrif', 'Haroon al-Afghani', 'Tayyab Agha', 'Khadija Zahra Ahmadi', 'Aziza Ahmadyar', 'Muqadasa Ahmadzai', 'Mohammad Sarwar Ahmedzai', 'Amir Muhammad Akhundzada', 'Nasrullah Baryalai Arsalai', 'Abdul Rahim Ayoubi']\n"
     ]
    }
   ],
   "source": [
    "# Load data from the CSV files\n",
    "politicians_by_country = pd.read_csv('politicians_by_country_AUG.2024.csv')\n",
    "pop_by_country = pd.read_csv('population_by_country_AUG.2024.csv')\n",
    "\n",
    "# Remove duplicates in the 'name' column (which is the first column)\n",
    "politicians_by_country_cleaned = politicians_by_country.iloc[:, 0].drop_duplicates()\n",
    "\n",
    "# Extract the article titles from the cleaned data (first column)\n",
    "ARTICLE_TITLES = politicians_by_country_cleaned.tolist()\n",
    "\n",
    "# Display the first 10 titles (just to verify the extraction)\n",
    "print(ARTICLE_TITLES[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "ORES_API_ENDPOINT = \"https://ores.wikimedia.org/v3/scores/{}/?models=draftquality&revids={}\"\n",
    "\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<kilpas@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2024'\n",
    "}\n",
    "\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers|lastrevid\"\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n",
    "\n",
    "MAX_TITLES_PER_REQUEST = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_pageinfo_per_articles(article_titles, \n",
    "                                  endpoint_url=API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                  request_template=PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                  headers=REQUEST_HEADERS):\n",
    "    \"\"\"\n",
    "    Request page info for a list of article titles in batches of 50.\n",
    "    Returns the page info data including the current revision ID.\n",
    "    \"\"\"\n",
    "    if 'uwnetid@uw.edu' in headers['User-Agent']:\n",
    "        raise Exception(\"Please replace 'uwnetid@uw.edu' with your actual UW email address in the headers.\")\n",
    "    \n",
    "    article_batches = [article_titles[i:i + MAX_TITLES_PER_REQUEST] for i in range(0, len(article_titles), MAX_TITLES_PER_REQUEST)]\n",
    "    all_pages_info = {}\n",
    "    \n",
    "    for batch in article_batches:\n",
    "        page_titles = \"|\".join(batch)\n",
    "        request_template['titles'] = page_titles\n",
    "        \n",
    "        try:\n",
    "            time.sleep(0.1)  # Throttle requests\n",
    "            response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "            json_response = response.json()\n",
    "            \n",
    "            if json_response and 'query' in json_response:\n",
    "                pages = json_response['query']['pages']\n",
    "                all_pages_info.update(pages)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching page info for titles: {page_titles}\\nError: {e}\")\n",
    "    \n",
    "    return all_pages_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politicians and their revision IDs have been saved to 'politicians-and-revision-ids.json'.\n"
     ]
    }
   ],
   "source": [
    "def request_pageinfo_per_articles(article_titles, \n",
    "                                   endpoint_url=API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                   request_template=PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                   headers=REQUEST_HEADERS):\n",
    "    \"\"\"\n",
    "    Request page info for a list of article titles in batches of 50.\n",
    "    Returns a dictionary with politician names as keys and their last revision IDs as values.\n",
    "    \"\"\"\n",
    "    if 'uwnetid@uw.edu' in headers['User-Agent']:\n",
    "        raise Exception(\"Please replace 'uwnetid@uw.edu' with your actual UW email address in the headers.\")\n",
    "    \n",
    "    article_batches = [article_titles[i:i + MAX_TITLES_PER_REQUEST] for i in range(0, len(article_titles), MAX_TITLES_PER_REQUEST)]\n",
    "    politicians_revisions = {}  # Dictionary to hold politician names and their last revision IDs\n",
    "    \n",
    "    for batch in article_batches:\n",
    "        page_titles = \"|\".join(batch)\n",
    "        request_template['titles'] = page_titles\n",
    "        \n",
    "        try:\n",
    "            time.sleep(0.1)  # Throttle requests\n",
    "            response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "            json_response = response.json()\n",
    "            \n",
    "            if json_response and 'query' in json_response:\n",
    "                pages = json_response['query']['pages']\n",
    "                for page_id, page_data in pages.items():\n",
    "                    politician_name = page_data.get('title')\n",
    "                    last_revision_id = page_data.get('lastrevid')\n",
    "                    if politician_name and last_revision_id:\n",
    "                        # Store only title and lastrevid in the output dictionary\n",
    "                        politicians_revisions[politician_name] = {\n",
    "                            'title': politician_name,\n",
    "                            'lastrevid': last_revision_id\n",
    "                        }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching page info for titles: {page_titles}\\nError: {e}\")\n",
    "    \n",
    "    return politicians_revisions\n",
    "\n",
    "# Example usage:\n",
    "# Load data from CSV files and prepare ARTICLE_TITLES as before\n",
    "# politicians_by_country = pd.read_csv('politicians_by_country_AUG.2024.csv')\n",
    "# ARTICLE_TITLES = politicians_by_country['name'].drop_duplicates().tolist()\n",
    "\n",
    "# Step 1: Get page info for each politician (including revision ID)\n",
    "politicians_revisions = request_pageinfo_per_articles(ARTICLE_TITLES)\n",
    "\n",
    "# Step 2: Write the data to a JSON file\n",
    "with open('politicians-and-revision-ids.json', 'w') as json_file:\n",
    "    json.dump(politicians_revisions, json_file, indent=4)\n",
    "\n",
    "print(\"Politicians and their revision IDs have been saved to 'politicians-and-revision-ids.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get page info for each politician (including revision ID)\n",
    "page_info_data = request_pageinfo_per_articles(ARTICLE_TITLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page info data for: Majah Ha Adrif|Tayyab Agha|Aziza Ahmadyar\n",
      "No data retrieved or error occurred.\n"
     ]
    }
   ],
   "source": [
    "# Use the first few titles as an example for making the page info request\n",
    "example_titles = [ARTICLE_TITLES[0], ARTICLE_TITLES[2], ARTICLE_TITLES[4]]\n",
    "# Join the titles with the pipe as the delimiter\n",
    "page_titles = \"|\".join(example_titles)\n",
    "\n",
    "# Print the titles being requested for clarity\n",
    "print(f\"Getting page info data for: {page_titles}\")\n",
    "\n",
    "# Create a copy of the PAGEINFO_PARAMS_TEMPLATE for the request\n",
    "request_info = PAGEINFO_PARAMS_TEMPLATE.copy()\n",
    "request_info['titles'] = page_titles\n",
    "\n",
    "# Make the page info request using the updated request_pageinfo_per_articles function\n",
    "info = request_pageinfo_per_articles(article_titles=example_titles)\n",
    "\n",
    "# Print the results of the request\n",
    "if info and 'query' in info:\n",
    "    print(json.dumps(info['query']['pages'], indent=4))\n",
    "else:\n",
    "    print(\"No data retrieved or error occurred.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'47805901': {'pageid': 47805901,\n",
       "  'ns': 0,\n",
       "  'title': 'Aziza Ahmadyar',\n",
       "  'contentmodel': 'wikitext',\n",
       "  'pagelanguage': 'en',\n",
       "  'pagelanguagehtmlcode': 'en',\n",
       "  'pagelanguagedir': 'ltr',\n",
       "  'touched': '2024-10-08T13:30:38Z',\n",
       "  'lastrevid': 1195651393,\n",
       "  'length': 3790,\n",
       "  'talkid': 47806200,\n",
       "  'fullurl': 'https://en.wikipedia.org/wiki/Aziza_Ahmadyar',\n",
       "  'editurl': 'https://en.wikipedia.org/w/index.php?title=Aziza_Ahmadyar&action=edit',\n",
       "  'canonicalurl': 'https://en.wikipedia.org/wiki/Aziza_Ahmadyar'},\n",
       " '10483286': {'pageid': 10483286,\n",
       "  'ns': 0,\n",
       "  'title': 'Majah Ha Adrif',\n",
       "  'contentmodel': 'wikitext',\n",
       "  'pagelanguage': 'en',\n",
       "  'pagelanguagehtmlcode': 'en',\n",
       "  'pagelanguagedir': 'ltr',\n",
       "  'touched': '2024-09-30T14:32:18Z',\n",
       "  'lastrevid': 1233202991,\n",
       "  'length': 3188,\n",
       "  'talkid': 13330265,\n",
       "  'fullurl': 'https://en.wikipedia.org/wiki/Majah_Ha_Adrif',\n",
       "  'editurl': 'https://en.wikipedia.org/w/index.php?title=Majah_Ha_Adrif&action=edit',\n",
       "  'canonicalurl': 'https://en.wikipedia.org/wiki/Majah_Ha_Adrif'},\n",
       " '46841383': {'pageid': 46841383,\n",
       "  'ns': 0,\n",
       "  'title': 'Tayyab Agha',\n",
       "  'contentmodel': 'wikitext',\n",
       "  'pagelanguage': 'en',\n",
       "  'pagelanguagehtmlcode': 'en',\n",
       "  'pagelanguagedir': 'ltr',\n",
       "  'touched': '2024-10-07T15:18:31Z',\n",
       "  'lastrevid': 1225661708,\n",
       "  'length': 6346,\n",
       "  'talkid': 46843786,\n",
       "  'fullurl': 'https://en.wikipedia.org/wiki/Tayyab_Agha',\n",
       "  'editurl': 'https://en.wikipedia.org/w/index.php?title=Tayyab_Agha&action=edit',\n",
       "  'canonicalurl': 'https://en.wikipedia.org/wiki/Tayyab_Agha'}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
